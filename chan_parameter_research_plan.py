#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chan.py å‚æ•°ç ”ç©¶è®¡åˆ’
===================

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„ç ”ç©¶æ¡†æ¶ï¼Œç”¨äºæ¢ç´¢Chan.pyä¸­å„ç§å‚æ•°è®¾ç½®çš„å¯èƒ½æ€§åŠå…¶ç»“æœã€‚

ç ”ç©¶ç›®æ ‡ï¼š
1. è¯†åˆ«å…³é”®å‚æ•°å¯¹ç¼ è®ºåˆ†æç»“æœçš„å½±å“
2. æ‰¾åˆ°æœ€ä¼˜å‚æ•°ç»„åˆ
3. ç†è§£ä¸åŒå‚æ•°è®¾ç½®çš„é€‚ç”¨åœºæ™¯
4. å»ºç«‹å‚æ•°è°ƒä¼˜çš„æ–¹æ³•è®º

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2024
"""

import pandas as pd
import numpy as np
from itertools import product
from typing import Dict, List, Any, Tuple
import json
import os
from datetime import datetime

class ChanParameterResearcher:
    """Chan.pyå‚æ•°ç ”ç©¶å™¨"""
    
    def __init__(self, data_path: str, output_dir: str = "research_results"):
        self.data_path = data_path
        self.output_dir = output_dir
        self.results = []
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
    def get_parameter_space(self) -> Dict[str, List[Any]]:
        """å®šä¹‰å‚æ•°ç©ºé—´"""
        return {
            # ç¬”å‚æ•°
            "bi_algo": ["normal", "fx"],
            "bi_strict": [True, False],
            "bi_fx_check": ["strict", "loss", "half", "totally"],
            "gap_as_kl": [True, False],
            "bi_end_is_peak": [True, False],
            "bi_allow_sub_peak": [True, False],
            
            # çº¿æ®µå‚æ•°
            "seg_algo": ["chan", "1+1", "break"],
            "left_seg_method": ["peak", "all"],
            
            # ä¸­æ¢å‚æ•°
            "zs_combine": [True, False],
            "zs_combine_mode": ["zs", "peak"],
            "one_bi_zs": [True, False],
            "zs_algo": ["normal", "over_seg", "auto"],
            
            # ä¹°å–ç‚¹å‚æ•°
            "divergence_rate": [0.5, 0.618, 0.8, 0.9, 1.0, float("inf")],
            "min_zs_cnt": [0, 1, 2, 3],
            "bsp1_only_multibi_zs": [True, False],
            "max_bs2_rate": [0.5, 0.618, 0.8, 0.9, 0.9999],
            "macd_algo": ["area", "peak", "full_area", "diff", "slope", "amp"],
            "bs1_peak": [True, False],
            "bsp2_follow_1": [True, False],
            "bsp3_follow_1": [True, False],
            "bsp3_peak": [True, False],
            "strict_bsp3": [True, False],
        }
    
    def get_core_parameter_combinations(self) -> List[Dict[str, Any]]:
        """è·å–æ ¸å¿ƒå‚æ•°ç»„åˆï¼ˆå‡å°‘ç»„åˆæ•°é‡ï¼‰"""
        core_params = {
            "zs_algo": ["normal", "over_seg"],
            "zs_combine": [True, False],
            "zs_combine_mode": ["zs", "peak"],
            "bi_strict": [True, False],
            "seg_algo": ["chan", "1+1"],
            "macd_algo": ["peak", "area", "slope"],
            "divergence_rate": [0.618, 0.9, float("inf")],
        }
        
        combinations = []
        for values in product(*core_params.values()):
            combo = dict(zip(core_params.keys(), values))
            combinations.append(combo)
        
        return combinations
    
    def get_evaluation_metrics(self) -> List[str]:
        """å®šä¹‰è¯„ä¼°æŒ‡æ ‡"""
        return [
            "bi_count",           # ç¬”æ•°é‡
            "seg_count",          # çº¿æ®µæ•°é‡
            "zs_count",           # ä¸­æ¢æ•°é‡
            "bsp1_count",         # 1ç±»ä¹°å–ç‚¹æ•°é‡
            "bsp2_count",         # 2ç±»ä¹°å–ç‚¹æ•°é‡
            "bsp3_count",         # 3ç±»ä¹°å–ç‚¹æ•°é‡
            "avg_bi_length",      # å¹³å‡ç¬”é•¿åº¦
            "avg_seg_length",     # å¹³å‡çº¿æ®µé•¿åº¦
            "avg_zs_length",      # å¹³å‡ä¸­æ¢é•¿åº¦
            "zs_coverage_ratio",  # ä¸­æ¢è¦†ç›–ç‡
            "trend_consistency",  # è¶‹åŠ¿ä¸€è‡´æ€§
            "signal_frequency",   # ä¿¡å·é¢‘ç‡
            "computation_time",   # è®¡ç®—æ—¶é—´
        ]
    
    def create_research_phases(self) -> Dict[str, Dict]:
        """åˆ›å»ºç ”ç©¶é˜¶æ®µ"""
        return {
            "phase1_core_exploration": {
                "description": "æ ¸å¿ƒå‚æ•°æ¢ç´¢",
                "parameters": ["zs_algo", "zs_combine", "bi_strict", "seg_algo"],
                "focus": "åŸºç¡€ç»“æ„å‚æ•°å¯¹åˆ†æç»“æœçš„å½±å“",
                "sample_size": 50
            },
            
            "phase2_zs_deep_dive": {
                "description": "ä¸­æ¢å‚æ•°æ·±åº¦ç ”ç©¶",
                "parameters": ["zs_algo", "zs_combine", "zs_combine_mode", "one_bi_zs"],
                "focus": "ä¸­æ¢è¯†åˆ«å’Œåˆå¹¶ç­–ç•¥çš„ä¼˜åŒ–",
                "sample_size": 100
            },
            
            "phase3_bsp_optimization": {
                "description": "ä¹°å–ç‚¹å‚æ•°ä¼˜åŒ–",
                "parameters": ["divergence_rate", "min_zs_cnt", "macd_algo", "max_bs2_rate"],
                "focus": "ä¹°å–ç‚¹è¯†åˆ«å‡†ç¡®æ€§å’Œå®ç”¨æ€§",
                "sample_size": 150
            },
            
            "phase4_bi_seg_tuning": {
                "description": "ç¬”æ®µå‚æ•°è°ƒä¼˜",
                "parameters": ["bi_fx_check", "bi_end_is_peak", "seg_algo", "left_seg_method"],
                "focus": "ç¬”æ®µè¯†åˆ«ç²¾åº¦å’Œç¨³å®šæ€§",
                "sample_size": 80
            },
            
            "phase5_comprehensive": {
                "description": "ç»¼åˆå‚æ•°æµ‹è¯•",
                "parameters": "all",
                "focus": "æœ€ä¼˜å‚æ•°ç»„åˆå‘ç°",
                "sample_size": 200
            }
        }
    
    def generate_test_scenarios(self) -> List[Dict]:
        """ç”Ÿæˆæµ‹è¯•åœºæ™¯"""
        scenarios = [
            {
                "name": "trending_market",
                "description": "è¶‹åŠ¿å¸‚åœº",
                "data_filter": "high_volatility_trend",
                "expected_behavior": "æ¸…æ™°çš„ç¬”æ®µç»“æ„ï¼Œè¾ƒå°‘ä¸­æ¢"
            },
            {
                "name": "sideways_market", 
                "description": "éœ‡è¡å¸‚åœº",
                "data_filter": "low_volatility_range",
                "expected_behavior": "è¾ƒå¤šä¸­æ¢ï¼Œé¢‘ç¹ä¹°å–ç‚¹"
            },
            {
                "name": "volatile_market",
                "description": "é«˜æ³¢åŠ¨å¸‚åœº", 
                "data_filter": "high_volatility_range",
                "expected_behavior": "å¤æ‚ç¬”æ®µç»“æ„ï¼Œå¤šå±‚çº§ä¸­æ¢"
            },
            {
                "name": "low_volume_market",
                "description": "ä½æˆäº¤é‡å¸‚åœº",
                "data_filter": "low_volume",
                "expected_behavior": "è¾ƒå°‘æœ‰æ•ˆä¿¡å·ï¼Œéœ€è¦å®½æ¾å‚æ•°"
            }
        ]
        return scenarios
    
    def create_parameter_sensitivity_analysis(self) -> Dict:
        """åˆ›å»ºå‚æ•°æ•æ„Ÿæ€§åˆ†æè®¡åˆ’"""
        return {
            "single_parameter_sweep": {
                "description": "å•å‚æ•°æ‰«æ",
                "method": "å›ºå®šå…¶ä»–å‚æ•°ï¼Œå•ç‹¬å˜åŒ–ä¸€ä¸ªå‚æ•°",
                "parameters": ["divergence_rate", "min_zs_cnt", "max_bs2_rate"]
            },
            
            "parameter_interaction": {
                "description": "å‚æ•°äº¤äº’ä½œç”¨",
                "method": "ä¸¤ä¸¤å‚æ•°ç»„åˆåˆ†æ",
                "key_pairs": [
                    ("zs_algo", "zs_combine"),
                    ("bi_strict", "bi_fx_check"),
                    ("seg_algo", "left_seg_method"),
                    ("macd_algo", "divergence_rate")
                ]
            },
            
            "stability_analysis": {
                "description": "å‚æ•°ç¨³å®šæ€§åˆ†æ",
                "method": "åœ¨ä¸åŒæ•°æ®æ®µä¸Šæµ‹è¯•ç›¸åŒå‚æ•°",
                "focus": "å‚æ•°è®¾ç½®çš„é²æ£’æ€§"
            }
        }
    
    def create_evaluation_framework(self) -> Dict:
        """åˆ›å»ºè¯„ä¼°æ¡†æ¶"""
        return {
            "quantitative_metrics": {
                "structure_metrics": [
                    "bi_count", "seg_count", "zs_count",
                    "avg_bi_length", "avg_seg_length"
                ],
                "signal_metrics": [
                    "bsp1_count", "bsp2_count", "bsp3_count",
                    "signal_frequency", "signal_quality"
                ],
                "performance_metrics": [
                    "computation_time", "memory_usage"
                ]
            },
            
            "qualitative_assessment": {
                "visual_inspection": [
                    "chart_clarity", "structure_reasonableness",
                    "signal_timing", "trend_identification"
                ],
                "theoretical_consistency": [
                    "chan_theory_compliance", "logical_coherence"
                ]
            },
            
            "comparative_analysis": {
                "baseline_comparison": "ä¸é»˜è®¤å‚æ•°å¯¹æ¯”",
                "cross_validation": "ä¸åŒæ—¶é—´æ®µéªŒè¯",
                "scenario_testing": "ä¸åŒå¸‚åœºç¯å¢ƒæµ‹è¯•"
            }
        }
    
    def save_research_plan(self):
        """ä¿å­˜ç ”ç©¶è®¡åˆ’"""
        research_plan = {
            "title": "Chan.py å‚æ•°ç ”ç©¶è®¡åˆ’",
            "created_at": datetime.now().isoformat(),
            "parameter_space": self.get_parameter_space(),
            "core_combinations": len(self.get_core_parameter_combinations()),
            "evaluation_metrics": self.get_evaluation_metrics(),
            "research_phases": self.create_research_phases(),
            "test_scenarios": self.generate_test_scenarios(),
            "sensitivity_analysis": self.create_parameter_sensitivity_analysis(),
            "evaluation_framework": self.create_evaluation_framework()
        }
        
        with open(f"{self.output_dir}/research_plan.json", "w", encoding="utf-8") as f:
            json.dump(research_plan, f, ensure_ascii=False, indent=2)
        
        print(f"ç ”ç©¶è®¡åˆ’å·²ä¿å­˜åˆ°: {self.output_dir}/research_plan.json")
        return research_plan

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç ”ç©¶è®¡åˆ’çš„ä½¿ç”¨"""
    
    # åˆ›å»ºç ”ç©¶å™¨å®ä¾‹
    researcher = ChanParameterResearcher(
        data_path="eth_data.csv",
        output_dir="eth_charts/research_results"
    )
    
    # ç”Ÿæˆå¹¶ä¿å­˜ç ”ç©¶è®¡åˆ’
    plan = researcher.save_research_plan()
    
    # æ‰“å°ç ”ç©¶æ¦‚è¦
    print("\n" + "="*60)
    print("Chan.py å‚æ•°ç ”ç©¶è®¡åˆ’æ¦‚è¦")
    print("="*60)
    
    print(f"\nğŸ“Š å‚æ•°ç©ºé—´å¤§å°:")
    param_space = researcher.get_parameter_space()
    total_combinations = 1
    for param, values in param_space.items():
        total_combinations *= len(values)
        print(f"  {param}: {len(values)} ä¸ªé€‰é¡¹")
    print(f"  æ€»ç»„åˆæ•°: {total_combinations:,}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒå‚æ•°ç»„åˆ: {len(researcher.get_core_parameter_combinations())} ä¸ª")
    
    print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡: {len(researcher.get_evaluation_metrics())} ä¸ª")
    for metric in researcher.get_evaluation_metrics():
        print(f"  - {metric}")
    
    print(f"\nğŸ”¬ ç ”ç©¶é˜¶æ®µ: {len(researcher.create_research_phases())} ä¸ª")
    for phase_name, phase_info in researcher.create_research_phases().items():
        print(f"  {phase_name}: {phase_info['description']}")
    
    print(f"\nğŸ­ æµ‹è¯•åœºæ™¯: {len(researcher.generate_test_scenarios())} ä¸ª")
    for scenario in researcher.generate_test_scenarios():
        print(f"  {scenario['name']}: {scenario['description']}")
    
    print("\n" + "="*60)
    print("å»ºè®®çš„ç ”ç©¶æ­¥éª¤:")
    print("="*60)
    print("1. ä» phase1_core_exploration å¼€å§‹")
    print("2. ä½¿ç”¨ ETH æ•°æ®è¿›è¡Œåˆæ­¥æµ‹è¯•")
    print("3. é‡ç‚¹å…³æ³¨ zs_algo å’Œ zs_combine å‚æ•°")
    print("4. è®°å½•æ¯ä¸ªå‚æ•°ç»„åˆçš„ç»“æœ")
    print("5. é€æ­¥æ·±å…¥åˆ°å…¶ä»–ç ”ç©¶é˜¶æ®µ")
    print("6. æœ€ç»ˆå½¢æˆå‚æ•°è°ƒä¼˜æŒ‡å—")

if __name__ == "__main__":
    main()